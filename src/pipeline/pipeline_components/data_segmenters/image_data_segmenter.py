from typing import List, Dict, Any
from .abstract_data_segmenter import AbstractDataSegmenter
from src.pipeline.data_entities.image_segmentation_mask_data_entity import ImageSegmentationMaskDataEntity

import numpy as np
import torch
import cv2
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../../segment_anything_2/sam2')))
from build_sam import build_sam2_camera_predictor, build_sam2
from automatic_mask_generator import SAM2AutomaticMaskGenerator

from collections import defaultdict
import time
from scipy.optimize import linear_sum_assignment

import uuid
import argparse
import hydra

class ImageDataSegmenter(AbstractDataSegmenter):
    """
    Component for segmenting image data using SAM2 with YOLO bounding box prompts.
    
    Args:
        model_cfg_path: SAM2 model configuration path
        checkpoint_path: SAM2 checkpoint path  
        detection_interval: Use YOLO detections every N frames (default: 1)
        min_mask_region_area: Minimum area for valid masks (default: 500)
        save_debug_images: Whether to save debug images (default: False)
        debug_output_dir: Directory to save debug images (default: "debug_segmentation")
    Returns:
        Dictionary containing image segmentation mask
    Raises:
        ValueError: If YOLO detections are not available when needed
    """

    def __init__(self,
                 model_cfg_path,
                 checkpoint_path,
                 detection_interval=1,
                 min_mask_region_area=500,
                 save_debug_images=False ,
                 debug_output_dir="debug_segmentation"
                 ) -> None:
        super().__init__()
        # DO NOT resolve model_cfg_path to absolute path! Pass as config name for Hydra
        # Only resolve checkpoint_path to absolute path if needed
        if not os.path.isabs(checkpoint_path):
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../../'))
            checkpoint_path = os.path.join(project_root, checkpoint_path)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.detection_interval = detection_interval
        self.min_mask_region_area = min_mask_region_area
        
        # Debug settings
        self.save_debug_images = save_debug_images
        self.debug_output_dir = debug_output_dir
        if self.save_debug_images:
            # Create organized subdirectories
            self.debug_dirs = {
                'original': os.path.join(self.debug_output_dir, 'original_images'),
                'yolo': os.path.join(self.debug_output_dir, 'yolo_detections'),
                'segmentation': os.path.join(self.debug_output_dir, 'segmentation_masks'),
                'overlays': os.path.join(self.debug_output_dir, 'overlays'),
                'individual_masks': os.path.join(self.debug_output_dir, 'individual_masks')
            }
            
            # Create all directories
            for dir_path in self.debug_dirs.values():
                os.makedirs(dir_path, exist_ok=True)
            
            print(f"Debug images will be saved to organized folders in: {self.debug_output_dir}")
            print("  - original_images/: Original input frames")
            print("  - yolo_detections/: Images with YOLO bounding boxes")
            print("  - segmentation_masks/: Colored segmentation masks")
            print("  - overlays/: Original images with mask overlays")
            print("  - individual_masks/: Individual object masks")
        
        # Build SAM2 predictor
        self.predictor = build_sam2_camera_predictor(model_cfg_path, checkpoint_path, device=self.device)
        
        # Tracking state
        self.last_detection_frame = -detection_interval
        self.track_history = defaultdict(int)  # Track lifetime of objects
        self.if_first_frame = False
        self.masks = {}
        self.previous_masks = {}

        #conversion of multi mask to single mask
        #0 segment means non-segmented, i.e. there was no mask generated by sam2 that contained that pixel
        self.max_segment_int = 0 #counter for counting the segments, corresponds to id of sam2 mapped to lowest int
        self.id_to_int = {}
        self.id_to_class_label = {}  # Map SAM2 object IDs to YOLO class labels



    @property
    def inputs_from_bucket(self) -> List[str]:
        """This component requires RGB images, step number, and YOLO detections as input."""
        return ["image", "step_nr", "yolo_detections"]
    
    @property
    def outputs_to_bucket(self) -> List[str]:
        """This component outputs segmentation masks and labels."""
        return ["image_segmentation_mask", "segmentation_labels"]
    
    def _run(self, image: Any, step_nr: int, yolo_detections: Dict[str, Any], **kwargs: Any) -> Dict[str, Any]:
        """
        Segment an RGB image using YOLO bounding boxes as prompts for SAM2.
        
        Args:
            image: The input RGB image
            step_nr: Current step/frame number
            yolo_detections: YOLO detection results with bounding boxes
            **kwargs: Additional unused arguments
        """
        rgb_image = image.as_numpy()
        # Convert normalized float image [0,1] to 0-255 uint8 if needed
        if rgb_image.dtype in [np.float32, np.float64] and rgb_image.max() <= 1.0:
            rgb_image = (rgb_image * 255).clip(0, 255).astype(np.uint8)
        elif rgb_image.dtype != np.uint8:
            rgb_image = np.clip(rgb_image, 0, 255).astype(np.uint8)
            
        # Check if we should initialize tracking with new YOLO detections
        if (step_nr - self.last_detection_frame >= self.detection_interval and 
            yolo_detections and len(yolo_detections) > 0):
            
            # Extract bounding boxes from YOLO detections
            bboxes, class_labels = self._extract_bboxes_from_yolo(yolo_detections)
            if bboxes:
                self._initialize_tracking(rgb_image, bboxes, class_labels)
                self.last_detection_frame = step_nr

        # Propagate masks to current frame
        self.masks = self._propagate_masks(rgb_image)
        # self.masks = self.match_new_masks(self.masks)

        
        #transfer the multiple segmentation masks into a single one
        self.mask_dic= {}
        segmentation_mask = np.zeros(rgb_image.shape[0:2],dtype=np.int32)
        current_segmentation_labels = {}  # Map segment integers to class labels for this frame
        
        for seg_id, seg_mask in self.masks.items():
            if seg_id in self.id_to_int.keys():
                seg_int = self.id_to_int[seg_id]
            else:
                self.max_segment_int += 1
                seg_int = self.max_segment_int 
                self.id_to_int[seg_id] = seg_int
            
            segmentation_mask[seg_mask] = seg_int
            self.mask_dic[seg_int] = seg_mask.astype(np.uint8)
            
            # Map the segment integer to class label if available
            if seg_id in self.id_to_class_label:
                current_segmentation_labels[seg_int] = self.id_to_class_label[seg_id]
        # print(self.mask_dic.keys())
        # print("here are the ids:")
        # print(np.unique(segmentation_mask))

        #print(segmentation_mask)
        # Save debug images if enabled
        if self.save_debug_images:
            self._save_debug_images(
                rgb_image, 
                step_nr, 
                yolo_detections, 
                segmentation_mask, 
                self.masks,
                current_segmentation_labels
            )
        
        print(np.unique(segmentation_mask))

        #convert to segmentation mask data entity
        segmentation_mask = ImageSegmentationMaskDataEntity(segmentation_mask, self.mask_dic)

        return {
            "image_segmentation_mask": segmentation_mask,
            "segmentation_labels": current_segmentation_labels
        }
    
    def _extract_bboxes_from_yolo(self, yolo_detections: Dict[str, Any]) -> tuple:
        """
        Extract bounding boxes and class labels from YOLO detection results.
        
        Args:
            yolo_detections: Dictionary with class names as keys and detection lists as values
            
        Returns:
            Tuple of (bboxes, class_labels) where bboxes is list of [x1, y1, x2, y2] 
            and class_labels is list of class names
        """
        bboxes = []
        class_labels = []
        
        for class_name, detections in yolo_detections.items():
            for detection in detections:
                bbox = detection.get("bbox")  # [x1, y1, x2, y2] format
                if bbox and len(bbox) == 4:
                    bboxes.append(bbox)
                    class_labels.append(class_name)
        
        return bboxes, class_labels

    def _save_debug_images(self, rgb_image, step_nr, yolo_detections, segmentation_mask, individual_masks, segmentation_labels=None):
        """
        Save debug images showing original image, YOLO boxes, segmentation masks, and overlays.
        """
        try:
            # 1. Save original image
            original_path = os.path.join(self.debug_dirs['original'], f"frame_{step_nr:04d}_original.jpg")
            cv2.imwrite(original_path, cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
            
            # 2. Save image with YOLO bounding boxes
            img_with_boxes = rgb_image.copy()
            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
            
            color_idx = 0
            for class_name, detections in yolo_detections.items():
                color = colors[color_idx % len(colors)]
                for detection in detections:
                    bbox = detection.get("bbox")
                    if bbox and len(bbox) == 4:
                        x1, y1, x2, y2 = [int(coord) for coord in bbox]
                        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 2)
                        
                        # Add class label and confidence
                        conf = detection.get("confidence", 0.0)
                        label = f"{class_name}: {conf:.2f}"
                        cv2.putText(img_with_boxes, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                color_idx += 1
            
            yolo_boxes_path = os.path.join(self.debug_dirs['yolo'], f"frame_{step_nr:04d}_yolo_detections.jpg")
            cv2.imwrite(yolo_boxes_path, cv2.cvtColor(img_with_boxes, cv2.COLOR_RGB2BGR))
            
            # 3. Save segmentation mask as colored image
            if segmentation_mask.max() > 0:
                # Create colored segmentation mask
                colored_mask = np.zeros((*segmentation_mask.shape, 3), dtype=np.uint8)
                unique_ids = np.unique(segmentation_mask)
                for i, seg_id in enumerate(unique_ids):
                    if seg_id == 0:  # Skip background
                        continue
                    color = colors[i % len(colors)]
                    colored_mask[segmentation_mask == seg_id] = color
                
                mask_path = os.path.join(self.debug_dirs['segmentation'], f"frame_{step_nr:04d}_segmentation_mask.jpg")
                cv2.imwrite(mask_path, cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR))
                
                # 4. Save overlay of original image + segmentation mask
                overlay = cv2.addWeighted(rgb_image, 0.7, colored_mask, 0.3, 0)
                overlay_path = os.path.join(self.debug_dirs['overlays'], f"frame_{step_nr:04d}_overlay.jpg")
                cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
                
                # 5. Save individual masks for each object
                for seg_id, mask in individual_masks.items():
                    if np.sum(mask) > 0:
                        # Convert boolean mask to 0-255
                        binary_mask = (mask * 255).astype(np.uint8)
                        # Get class label if available for better filename
                        class_label = segmentation_labels.get(self.id_to_int.get(seg_id, 0), "unknown") if segmentation_labels else "unknown"
                        individual_path = os.path.join(self.debug_dirs['individual_masks'], f"frame_{step_nr:04d}_mask_{seg_id}_{class_label}.jpg")
                        cv2.imwrite(individual_path, binary_mask)
            
            # 6. Save segmentation labels summary as JSON
            if segmentation_labels:
                import json
                labels_summary = {
                    "frame": step_nr,
                    "segmentation_labels": segmentation_labels,
                    "yolo_detections": {
                        class_name: len(detections) 
                        for class_name, detections in yolo_detections.items()
                    },
                    "total_segments": len(segmentation_labels),
                    "total_yolo_detections": sum(len(detections) for detections in yolo_detections.values())
                }
                
                labels_path = os.path.join(self.debug_dirs['segmentation'], f"frame_{step_nr:04d}_labels.json")
                with open(labels_path, 'w') as f:
                    json.dump(labels_summary, f, indent=2)
            
            # Print debug info
            print(f"Debug images saved for frame {step_nr}:")
            print(f"  - YOLO detections: {len(yolo_detections)} classes")
            print(f"  - SAM masks: {len(individual_masks)} objects")
            print(f"  - Unique segments: {len(np.unique(segmentation_mask)) - 1}")  # -1 for background
            if segmentation_labels:
                print(f"  - Segmentation labels: {segmentation_labels}")
            
        except Exception as e:
            print(f"Error saving debug images for frame {step_nr}: {e}")

    def batch_iou(self, new_masks, old_masks):
        if not new_masks or not old_masks:
            return np.zeros((len(new_masks), len(old_masks)))
        new_stack = np.stack(new_masks)
        old_stack = np.stack(old_masks)
        inter = np.tensordot(new_stack, old_stack, axes=([1,2],[1,2]))
        union = new_stack.sum((1,2))[:, None] + old_stack.sum((1,2))[None, :] - inter
        print("Intersection matrix:\n", inter)
        print("Union matrix:\n", union)

        # --- Save intersection and union images for each pair ---
        import cv2
        import os
        debug_dir = getattr(self, 'debug_dirs', None)
        if debug_dir is not None and 'iou' not in debug_dir:
            debug_dir['iou'] = os.path.join(self.debug_output_dir, 'iou')
            os.makedirs(debug_dir['iou'], exist_ok=True)
        save_dir = debug_dir['iou'] if debug_dir is not None else './iou_debug'
        os.makedirs(save_dir, exist_ok=True)
        # Save normal masks
        for i in range(new_stack.shape[0]):
            cv2.imwrite(os.path.join(save_dir, f'new_mask_{i}.png'), new_stack[i].astype(np.uint8) * 255)
        for j in range(old_stack.shape[0]):
            cv2.imwrite(os.path.join(save_dir, f'old_mask_{j}.png'), old_stack[j].astype(np.uint8) * 255)
        # Save intersection and union
        for i in range(new_stack.shape[0]):
            for j in range(old_stack.shape[0]):
                inter_mask = (new_stack[i] & old_stack[j]).astype(np.uint8) * 255
                union_mask = (new_stack[i] | old_stack[j]).astype(np.uint8) * 255
                cv2.imwrite(os.path.join(save_dir, f'inter_mask_{i}_{j}.png'), inter_mask)
                cv2.imwrite(os.path.join(save_dir, f'union_mask_{i}_{j}.png'), union_mask)
        # --- End save ---

        return inter / (union + 1e-6)

    def match_new_masks(self, new_masks, iou_thresh=0.1):

        print(f"previous keys {self.previous_masks.keys()}")
        print(f"previous keys {new_masks.keys()}")

        old_ids = list(self.previous_masks.keys())
        new_ids = list(new_masks.keys())
        old_masks = [(self.previous_masks[k] > 0).astype(np.uint8) for k in old_ids]
        new_masks_bin = [(m > 0).astype(np.uint8) for m in new_masks.values()]

        # Debug: print mask shapes and sums
        for i, m in enumerate(new_masks_bin):
            print(f"New mask {i}: shape={m.shape}, dtype={m.dtype}, unique={np.unique(m)}, sum={np.sum(m)}")
        for i, m in enumerate(old_masks):
            print(f"Old mask {i}: shape={m.shape}, dtype={m.dtype}, unique={np.unique(m)}, sum={np.sum(m)}")

        iou = self.batch_iou(new_masks_bin, old_masks)
        print("IOU matrix:\n", iou)

        cost = -iou
        row, col = linear_sum_assignment(cost) if old_ids else ([], [])

        matches = {}
        matched = set()
        for i, j in zip(row, col):
            print(f"Matching new {i} to old {j} with IOU {iou[i, j]}")
            if iou[i, j] >= iou_thresh:
                matches[old_ids[j]] = new_masks[new_ids[i]]
                matched.add(i)
        print(matches.keys())
        for i, key in enumerate(new_ids):
            if i not in matched:
                matches[key] = new_masks[key]
        print(matches.keys())

        self.previous_masks = matches  
        return matches

    def _initialize_tracking(self, frame, bboxes, class_labels):
        """Initialize tracking with YOLO bounding boxes"""
        # Reset inference state
        if not self.if_first_frame:
            self.predictor.load_first_frame(frame)
            self.if_first_frame = True
            frame_idx = 0
        else: 
            self.predictor.reset_state()
            self.predictor.add_conditioning_frame(frame)
            self.predictor.condition_state["tracking_has_started"] = False
            frame_idx = self.predictor.condition_state["num_frames"] - 1
            
        # Add each YOLO bounding box as a prompt for SAM2
        for bbox, class_label in zip(bboxes, class_labels):
            obj_id = str(uuid.uuid4())
            # Store the mapping between object ID and class label
            self.id_to_class_label[obj_id] = class_label
            
            self.predictor.add_new_prompt(
                frame_idx=frame_idx,
                obj_id=obj_id,
                points=None,
                bbox=bbox,
                clear_old_points=True,
                normalize_coords=True
            )
    
    def _propagate_masks(self, frame):
        """Propagate masks to current frame"""
        # Propagate masks
        out_obj_ids, out_mask_logits = self.predictor.track(frame)
        
        # Process outputs
        current_masks = {}
        for obj_id, mask_logits in zip(out_obj_ids, out_mask_logits):
            mask = (mask_logits > 0.0).cpu().numpy().squeeze()
            if np.sum(mask) > self.min_mask_region_area:  # Minimum area threshold
                current_masks[obj_id] = mask

        return current_masks
