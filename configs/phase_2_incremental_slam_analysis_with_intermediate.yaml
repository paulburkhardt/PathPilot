pipeline:
  components:
    # Load video data for SLAM processing
    - type: MAST3RSLAMVideoDataset
      config:
        video_path: Data/Videos/two_chairs_and_trash.mp4
        mast3r_slam_config_path: "configs/MASt3R-SLAM_configs/no_calib.yaml"

    # Run the yolo segmenter
    - type: YOLOSegmenter
      config:
        model_path: "models/yolov8s-oiv7.pt"
        detection_interval: 1  # Run YOLO every 5 frames for performance
        conf_threshold: 0.3  # Confidence threshold for detections
        iou_threshold: 0.5    # IoU threshold for NMS
        max_detections: 20    # Maximum detections per image
    
    # Run SAM2 segmentation using YOLO bounding boxes as prompts
    - type: ImageDataSegmenter
      config:
        model_cfg_path: "sam2.1/sam2.1_hiera_t.yaml"
        checkpoint_path: "segment_anything_2/checkpoints/sam2.1_hiera_tiny.pt"
        detection_interval: 1    # Use YOLO detections every 5 frames (matches YOLO interval)
        min_mask_region_area: 200  # Minimum area for valid masks
        save_debug_images: true  # Enable debug image saving
        debug_output_dir: "debug_phase2_segmentation"  # Directory for debug images


    # Run MAST3R SLAM to generate point cloud and trajectory incrementally
    - type: MAST3RSLAMComponent
      config:
        point_cloud_method: accumulating
        c_confidence_threshold: 1.5 
        mast3r_slam_config_path: "configs/MASt3R-SLAM_configs/no_calib.yaml"
        segment_point_cloud: True

    - type: PointCloudFilterSegmenter
      config:
        downsample_rate : 1
        min_cluster_size: 1000
        neighbor_distance:  0.05
        ignore_backround: True 

    - type: ImageEmbeddingSegmenter
      config:
        text: "Describe the object: " # the prompt to generat


    - type: BBoxObjectDatabase
      config: 
        match_threshold : 0.3 #
        embedding_weight : 0.8
        running_embedding_weight  :  0.7 
        knn_count  :  4
        use_blip  :  True 
        use_fusion  :  False
        enable_same_mask_tracking : False
        track_points: True



    
    # Incremental floor detection - waits for 3 frames, refines frequently for better accuracy
    - type: IncrementalFloorDetectionComponent
      config:
        min_frames: 3  # Wait for 3 frames before starting
        sample_ratio: 0.15  # Slightly more points for better accuracy
        ransac_threshold: 0.05
        min_inliers: 1000
        floor_threshold: 0.05
        refine_interval: 10  # Refine every 10 frames for better accuracy
        max_refinement_poses: 30  # Use last 30 poses for refinement
    
    # Find closest points incrementally - waits for floor detection
    - type: IncrementalClosestPointFinderComponent
      config:
        use_view_cone: true  # Set to true to enable view cone filtering
        cone_angle_deg: 90.0
        
    # Enhanced output writer with intermediate saving enabled
    - type: EnhancedSLAMOutputWriter
      config:
        output_dir: "enhanced_slam_outputs"
        output_name: "incremental_analysis_detailed"
        save_point_cloud: true
        save_trajectory: true
        save_floor_data: true
        save_closest_points: true
        save_yolo_detections: true  # Save YOLO detection results
        save_intermediate: true  # Save intermediate results
        intermediate_interval: 10   # Save intermediate results every 5 frames
        create_timestamped_dir: true
        analysis_format: "csv"  # Can be "json" or "csv" 